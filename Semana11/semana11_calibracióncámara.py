# -*- coding: utf-8 -*-
"""Semana11_CalibraciónCámara.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rkNJ6E7qhmOz7EqeRp4qbrlx1ygttd6c

## 1. Realizar la calibración de cámara de su celular
https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html
"""

# Se importan las librerías necesarias
import numpy as np
import cv2 as cv
import os
import glob
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt

# Código para verificar la calidad de las imagenes del tablero de ajedrez
def verificar_calidad_imagen(image, img_path, min_resolution=(800, 600), nitidez_umbral=100):
    # Verifica la resolución de la imagen
    height, width = image.shape[:2]
    if width < min_resolution[0] or height < min_resolution[1]:
        print(f"Resolución baja: {width}x{height}. Mínimo recomendado: {min_resolution[0]}x{min_resolution[1]}. Eliminando imagen {img_path}.")
        os.remove(img_path)  # Eliminar la imagen
        return False

    # Verifica la nitidez
    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)
    laplacian_var = cv.Laplacian(gray, cv.CV_64F).var()

    if laplacian_var < nitidez_umbral:
        print(f"Imagen borrosa (nitidez baja). Varianza del Laplaciano: {laplacian_var:.2f}. Umbral recomendado: {nitidez_umbral}. Eliminando imagen {img_path}.")
        os.remove(img_path)  # Eliminar la imagen
        return False

    # Si ambos criterios se cumplen, la imagen tiene buena resolución y nitidez y se mantiene
    print(f"Imagen aceptable. Resolución: {width}x{height}, Nitidez: {laplacian_var:.2f}.")
    return True

# Ruta a la carpeta de las imágenes de calibración
folder_path = '/content/drive/MyDrive/ComputerVision/Ejercicios/Semana11/ImagenesAjedrez/'

# Itera sobre las imágenes en la carpeta
for img_file in os.listdir(folder_path):
    img_path = os.path.join(folder_path, img_file)

    # Carga la imagen
    image = cv.imread(img_path)

    if image is not None:
        print(f"Revisando imagen: {img_file}")
        verificar_calidad_imagen(image, img_path)
    else:
        print(f"Error cargando la imagen: {img_path}")

# Define criterios de terminación para la refinación de esquinas
criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)

# Dimensiones del tablero de ajedrez (7x6 en este caso)
chessboard_size = (7, 6)

# Prepara los puntos 3D del tablero (0,0,0), (1,0,0), ..., (6,5,0)
objp = np.zeros((chessboard_size[0] * chessboard_size[1], 3), np.float32)
objp[:, :2] = np.mgrid[0:chessboard_size[0], 0:chessboard_size[1]].T.reshape(-1, 2)

# Arrays para almacenar puntos 3D (en espacio real) y 2D (en el plano de la imagen)
objpoints = []
imgpoints = []

calibration_images = glob.glob(os.path.join(folder_path, '*.jpg'))

for img_file in calibration_images:
    img = cv.imread(img_file)
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)

    # Encuentra las esquinas del tablero de ajedrez
    ret, corners = cv.findChessboardCorners(gray, chessboard_size, None)

    if ret:
        objpoints.append(objp)

        # Refina las esquinas encontradas
        corners2 = cv.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
        imgpoints.append(corners2)

        # Dibuja y muestra las esquinas del tablero
        cv.drawChessboardCorners(img, chessboard_size, corners2, ret)
        cv2_imshow(img)  # Muestra la imagen con las esquinas

# Calibra la cámara con los puntos obtenidos
ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)

# Guarda los parámetros de la cámara
print(f"Matriz de la cámara:\n{mtx}")
print(f"Coeficientes de distorsión:\n{dist}")

# Error de reproyección
mean_error = 0
for i in range(len(objpoints)):
    imgpoints2, _ = cv.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)
    error = cv.norm(imgpoints[i], imgpoints2, cv.NORM_L2)/len(imgpoints2)
    mean_error += error

print( "total error: {}".format(mean_error/len(objpoints)) )

"""## 2. Reconstruir los puntos 3D de un objeto seleccionado por usted"""

# Definir la carpeta de las imágenes del objeto
folder_object = '/content/drive/MyDrive/ComputerVision/Ejercicios/Semana11/Objeto/'

# Lista para almacenar las imágenes
object_images = []

# Iterar sobre las imágenes en la carpeta y cargarlas
for img_file in os.listdir(folder_object):
    img_path = os.path.join(folder_object, img_file)  # Construir la ruta completa
    img = cv.imread(img_path)  # Leer la imagen directamente
    if img is not None:  # Verificar si la imagen se cargó correctamente
        object_images.append(img)

# Crear un detector de puntos clave y un descriptor (ORB)
orb = cv.ORB_create()

# Inicializar las listas para los puntos clave y descriptores
keypoints_all = []
descriptors_all = []

# Extraer los puntos clave y descriptores de las imágenes del objeto
for img in object_images:
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    keypoints, descriptors = orb.detectAndCompute(gray, None)
    keypoints_all.append(keypoints)
    descriptors_all.append(descriptors)

# Crear el emparejador de descriptores
bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)

# Seleccionar dos imágenes para la reconstrucción básica
img1 = object_images[0]
img2 = object_images[1]

# Emparejar los descriptores entre las dos imágenes
matches = bf.match(descriptors_all[0], descriptors_all[1])
matches = sorted(matches, key=lambda x: x.distance)

# Dibujar los mejores emparejamientos entre las dos imágenes con líneas más gruesas
img_matches = cv.drawMatches(
    img1, keypoints_all[0], img2, keypoints_all[1], matches[:50], None,
    flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,
    matchColor=(0, 255, 0),  # Color de las líneas de correspondencia (verde)
    singlePointColor=(0, 0, 255),  # Color de los puntos clave (rojo)
    matchesThickness=2  # Grosor de las líneas
)
plt.imshow(cv.cvtColor(img_matches, cv.COLOR_BGR2RGB))
plt.show()

# Obtener los puntos emparejados
pts1 = np.float32([keypoints_all[0][m.queryIdx].pt for m in matches])
pts2 = np.float32([keypoints_all[1][m.trainIdx].pt for m in matches])

# Calcular la matriz esencial para la reconstrucción 3D
E, mask = cv.findEssentialMat(pts1, pts2, mtx, method=cv.RANSAC, prob=0.999, threshold=1.0)

# Recuperar la rotación y la traslación entre las dos imágenes
_, R, t, mask = cv.recoverPose(E, pts1, pts2, mtx)

# Proyección de los puntos 3D
points_3D = cv.triangulatePoints(np.hstack((np.eye(3), np.zeros((3, 1)))), np.hstack((R, t)), pts1.T, pts2.T)
points_3D /= points_3D[3]  # Normalizar los puntos 3D
points_3D = points_3D[:3].T

def bundle_adjustment(points_3D, pts1, pts2, K, R, t):
    """
    Realiza la optimización conjunta de puntos 3D y parámetros extrínsecos,
    con la reducción de error de re-proyección cuadrático.
    """

    # Función para proyectar los puntos 3D en las imágenes utilizando la matriz intrínseca y extrínseca
    def project_points(points_3D, R, t):
        """
        Proyecta los puntos reconstruidos con K: matriz intrínseca
        y con la matriz extrínseca [R | T].
        """
        points_3D_h = np.hstack((points_3D, np.ones((points_3D.shape[0], 1)))).T  # Convertir a homogéneo
        P = K @ np.hstack((R, t))  # Matriz de proyección
        projected_2D = (P @ points_3D_h)  # Proyección
        projected_2D /= projected_2D[2]  # Normalizar por la coordenada homogénea
        return projected_2D[:2].T  # Retornar las coordenadas 2D

    # Función para calcular el error de re-proyección
    def reprojection_error(params, points_3D, pts1, pts2, K):
        """
        Calcula el error de re-proyección entre los puntos proyectados y las coordenadas de imagen.
        """
        rvec = params[:3]  # Los primeros 3 parámetros corresponden al vector de rotación
        tvec = params[3:6]  # Los siguientes 3 parámetros corresponden al vector de traslación
        points_3D = points_3D.reshape(-1, 3)  # Reshape de los puntos 3D

        # Convertir el vector de rotación en una matriz de rotación
        R, _ = cv2.Rodrigues(rvec)
        t = tvec.reshape(3, 1)  # Convertir tvec en matriz columna

        # Proyectar los puntos 3D desde la cámara 1 (sin rotación ni traslación)
        projected_pts1 = project_points(points_3D, np.eye(3), np.zeros((3, 1)))

        # Proyectar los puntos 3D desde la cámara 2 (con rotación y traslación)
        projected_pts2 = project_points(points_3D, R, t)

        # Error de re-proyección para la imagen 1 y la imagen 2
        error_1 = (pts1 - projected_pts1).ravel()
        error_2 = (pts2 - projected_pts2).ravel()

        # Unir ambos errores
        return np.hstack((error_1, error_2))

    # Inicialización de los parámetros (vector de rotación y traslación)
    rvec, _ = cv2.Rodrigues(R)
    initial_params = np.hstack((rvec.ravel(), t.ravel()))

    # Resolver el sistema de ecuaciones no lineales utilizando la optimización de mínimos cuadrados
    result = least_squares(reprojection_error, initial_params, verbose=1, args=(points_3D, pts1, pts2, K))

    # Obtener los parámetros optimizados
    optimized_rvec = result.x[:3]
    optimized_t = result.x[3:6]
    optimized_R, _ = cv2.Rodrigues(optimized_rvec)

    return points_3D, optimized_R, optimized_t

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import least_squares

# Ruta a imágenes
PATH_IM1 = '/content/drive/MyDrive/ComputerVision/Ejercicios/Semana11/Objeto/1.jpg'
PATH_IM2 = '/content/drive/MyDrive/ComputerVision/Ejercicios/Semana11/Objeto/2.jpg'

# Número de puntos claves
NO_KPT = 1000

# Parámetrs intrínsecos del celular calculados con la calibración de la cámara
FX = 3.53302710e+03
FY = 3.54003934e+03
CX = 1.70925689e+03
CY = 1.75681967e+03

# Función de optimización
def bundle_adjustment(points_3D, pts1, pts2, K, R, t):
    def project_points(points_3D, R, t):
        points_3D_h = np.hstack((points_3D, np.ones((points_3D.shape[0], 1))))
        P = K @ np.hstack((R, t))
        projected_2D = (P @ points_3D_h.T).T
        projected_2D /= projected_2D[:, 2:3]
        return projected_2D[:, :2]

    def reprojection_error(params, points_3D, pts1, pts2, K):
        """Calcula el error de re-proyección"""
        rvec = params[:3]
        tvec = params[3:6]
        points_3D = points_3D.reshape(-1, 3)

        R, _ = cv2.Rodrigues(rvec)
        t = tvec.reshape(3, 1)

        projected_pts1 = project_points(points_3D, np.eye(3), np.zeros((3, 1)))
        projected_pts2 = project_points(points_3D, R, t)

        error_1 = (pts1 - projected_pts1).ravel()
        error_2 = (pts2 - projected_pts2).ravel()

        return np.hstack((error_1, error_2))

    rvec, _ = cv2.Rodrigues(R)
    initial_params = np.hstack((rvec.ravel(), t.ravel()))

    result = least_squares(reprojection_error, initial_params, verbose=1,
                           args=(points_3D, pts1, pts2, K))

    optimized_rvec = result.x[:3]
    optimized_t = result.x[3:6]
    optimized_R, _ = cv2.Rodrigues(optimized_rvec)

    return points_3D, optimized_R, optimized_t

def export_ply(filename, points_3D, colors):
    assert points_3D.shape[0] == colors.shape[0], "Los puntos y los colores deben tener el mismo número de entradas"

    # Open file to write
    with open(filename, 'w') as file:
        # Write PLY header
        file.write("ply\n")
        file.write("format ascii 1.0\n")
        file.write(f"element vertex {len(points_3D)}\n")
        file.write("property float x\n")
        file.write("property float y\n")
        file.write("property float z\n")
        file.write("property uchar red\n")
        file.write("property uchar green\n")
        file.write("property uchar blue\n")
        file.write("end_header\n")

        # Write the points and colors
        for point, color in zip(points_3D, colors):
            r, g, b = (color * 255).astype(int)
            file.write(f"{point[0]} {point[1]} {point[2]} {r} {g} {b}\n")

    print(f"PLY file written to {filename}")

# Cargar imágenes
img1 = cv2.imread(PATH_IM1)
img2 = cv2.imread(PATH_IM2)

# Matriz con parámetros intrínsecos
K = np.array([[FX, 0, CX], [0, FY, CY], [0, 0, 1]])

# Extracción y correspondencia de características
# Identificar puntos claves
orb = cv2.ORB_create(nfeatures=NO_KPT)
kp1, des1 = orb.detectAndCompute(img1, None)
kp2, des2 = orb.detectAndCompute(img2, None)

# Dibujar los puntos clave de ambas imágenes
img1_kpts = cv2.drawKeypoints(img1, kp1, None)
img2_kpts = cv2.drawKeypoints(img2, kp2, None)

# Mostrar puntos clave
plt.subplot(1, 2, 1)
plt.imshow(img1_kpts)
plt.title("Puntos clave de Imagen 1")

plt.subplot(1, 2, 2)
plt.imshow(img2_kpts)
plt.title("Puntos clave de Imagen 2")
plt.show()

# Emparejar los descriptores utilizando el BFMatcher con knnMatch
bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)
matches = bf.knnMatch(des1, des2, k=2)  # Aquí k=2 indica que obtendrás dos coincidencias para cada punto clave

# Apply Lowe's ratio test
good_matches = []
for match in matches:
    if len(match) == 2:  # Asegurarse de que hay al menos 2 coincidencias para aplicar la razón de Lowe
        m, n = match
        if m.distance < 0.75 * n.distance:  # Lowe's ratio test
            good_matches.append(m)

# Ordenar los emparejamientos por distancia (opcional)
good_matches = sorted(good_matches, key=lambda x: x.distance)

# Dibujar la imagen con las correspondencias
img_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

# Mostrar la imagen con las correspondencias
plt.imshow(img_matches)
plt.title("Correspondencia entre puntos clave")
plt.show()

# Obtener los colores de los puntos claves
colors = []
for match in good_matches:
    pt1 = kp1[match.queryIdx].pt
    color1 = img1[int(pt1[1]), int(pt1[0])] / 255.0
    colors.append(color1)

# Matriz Esencial entre las dos imágenes
pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, prob=0.999, threshold=0.5)  # Reduced threshold

_, R, t, _ = cv2.recoverPose(E, pts1, pts2, K, mask=mask)

# Triangulación de puntos
P1 = np.hstack((np.eye(3), np.zeros((3, 1))))
P2 = np.hstack((R, t))

P1 = K @ P1
P2 = K @ P2

points_4D = cv2.triangulatePoints(P1, P2, pts1, pts2)
points_3D = points_4D / points_4D[3]  # Convert from homogeneous to Cartesian coordinates
points_3D = points_3D[:3, :].T
points_3D, optimized_R, optimized_t = bundle_adjustment(points_3D, pts1, pts2, K, R, t)

# Guardar en formato PLY
export_ply('point_cloud.ply', points_3D, np.array(colors))

# Visualización 3D
points_3D = np.vstack(points_3D)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plot the 3D points
ax.scatter(points_3D[:, 0], points_3D[:, 1], points_3D[:, 2], marker='o', s=5, c=colors, alpha=0.5)

# Configure the plot
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
plt.show()